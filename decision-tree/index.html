



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Dokumentasi Data Mining">
      
      
        <link rel="canonical" href="https://ainunnawawi.github.io/170441100110/decision-tree/">
      
      
        <meta name="author" content="Ainun Nawawi">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>Decision Tree - Data Mining</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#klasifikasi-pohon-keputusan-dengan-python" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://ainunnawawi.github.io/170441100110" title="Data Mining" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Data Mining
            </span>
            <span class="md-header-nav__topic">
              Decision Tree
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/ainunnawawi/170441100110" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ainunnawawi/170441100110
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="K-Nearest Neighbors" class="md-tabs__link md-tabs__link--active">
        K-Nearest Neighbors
      </a>
    
  </li>

      
        
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://ainunnawawi.github.io/170441100110" title="Data Mining" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Data Mining
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/ainunnawawi/170441100110" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ainunnawawi/170441100110
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="K-Nearest Neighbors" class="md-nav__link">
      K-Nearest Neighbors
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Decision Tree
      </label>
    
    <a href="./" title="Decision Tree" class="md-nav__link md-nav__link--active">
      Decision Tree
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#algoritma-pohon-keputusan" title="Algoritma Pohon Keputusan" class="md-nav__link">
    Algoritma Pohon Keputusan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bagaimana-cara-kerja-algoritma-pohon-keputusan" title="Bagaimana cara kerja algoritma Pohon Keputusan?" class="md-nav__link">
    Bagaimana cara kerja algoritma Pohon Keputusan?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pengukuran-pilihan-atribut" title="Pengukuran Pilihan Atribut" class="md-nav__link">
    Pengukuran Pilihan Atribut
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keuntungan-informasi" title="Keuntungan Informasi" class="md-nav__link">
    Keuntungan Informasi
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rasio-keuntungan" title="Rasio Keuntungan" class="md-nav__link">
    Rasio Keuntungan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#indeks-gini" title="Indeks gini" class="md-nav__link">
    Indeks gini
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../penyusun/" title="Penyusun" class="md-nav__link">
      Penyusun
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#algoritma-pohon-keputusan" title="Algoritma Pohon Keputusan" class="md-nav__link">
    Algoritma Pohon Keputusan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bagaimana-cara-kerja-algoritma-pohon-keputusan" title="Bagaimana cara kerja algoritma Pohon Keputusan?" class="md-nav__link">
    Bagaimana cara kerja algoritma Pohon Keputusan?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pengukuran-pilihan-atribut" title="Pengukuran Pilihan Atribut" class="md-nav__link">
    Pengukuran Pilihan Atribut
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keuntungan-informasi" title="Keuntungan Informasi" class="md-nav__link">
    Keuntungan Informasi
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rasio-keuntungan" title="Rasio Keuntungan" class="md-nav__link">
    Rasio Keuntungan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#indeks-gini" title="Indeks gini" class="md-nav__link">
    Indeks gini
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="klasifikasi-pohon-keputusan-dengan-python">Klasifikasi Pohon Keputusan dengan Python<a class="headerlink" href="#klasifikasi-pohon-keputusan-dengan-python" title="Permanent link">&para;</a></h1>
<p>Dalam tutorial ini, pelajari Decision Tree Classification, langkah-langkah pemilihan atribut, dan bagaimana membangun dan mengoptimalkan Decision Tree Classifier menggunakan paket Python Scikit-learn.</p>
<p>Sebagai manajer pemasaran, Anda menginginkan sekumpulan pelanggan yang kemungkinan besar akan membeli produk Anda. Ini adalah bagaimana Anda dapat menghemat anggaran pemasaran Anda dengan menemukan audiens Anda.Sebagai manajer pinjaman, Anda perlu mengidentifikasi aplikasi pinjaman berisiko untuk mencapai tingkat gagal bayar pinjaman yang lebih rendah. Proses mengklasifikasikan pelanggan ke dalam kelompok pelanggan potensial dan non-potensial atau aplikasi pinjaman yang aman atau berisiko dikenal sebagai masalah klasifikasi. Klasifikasi adalah proses dua langkah, langkah belajar dan langkah prediksi. Pada langkah pembelajaran, model dikembangkan berdasarkan data pelatihan yang diberikan. Pada langkah prediksi, model digunakan untuk memprediksi respons untuk data yang diberikan. Decision Tree adalah salah satu algoritma klasifikasi termudah dan populer untuk dipahami dan ditafsirkan. Ini dapat digunakan untuk masalah klasifikasi dan regresi.</p>
<p>Dalam tutorial ini, Anda akan membahas topik-topik berikut:</p>
<ul>
<li>Algoritma Pohon Keputusan</li>
<li>Bagaimana cara kerja algoritma Pohon Keputusan?</li>
<li>Pengukuran Pilihan Atribut</li>
<li>Keuntungan Informasi</li>
<li>Rasio Keuntungan</li>
<li>Indeks gini</li>
<li>Mengoptimalkan Kinerja Pohon Keputusan</li>
<li>Bangunan Classifier di Scikit-learn</li>
<li>Pro dan kontra</li>
<li>Kesimpulan</li>
</ul>
<h2 id="algoritma-pohon-keputusan">Algoritma Pohon Keputusan<a class="headerlink" href="#algoritma-pohon-keputusan" title="Permanent link">&para;</a></h2>
<p>Pohon keputusan adalah struktur pohon seperti bagan di mana simpul internal mewakili fitur (atau atribut), cabang mewakili aturan keputusan, dan setiap simpul daun mewakili hasilnya. Node paling atas dalam pohon keputusan dikenal sebagai simpul akar. Ia belajar mempartisi berdasarkan nilai atribut. Ini partisi pohon dengan cara rekursif panggilan partisi rekursif. Struktur seperti bagan alur ini membantu Anda dalam pengambilan keputusan. Ini visualisasi seperti diagram alur yang dengan mudah meniru pemikiran tingkat manusia. Itulah sebabnya pohon keputusan mudah dipahami dan ditafsirkan.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\1.JPG" /></p>
<p>Pohon Keputusan adalah jenis kotak putih dari algoritma ML. Ini berbagi logika pengambilan keputusan internal, yang tidak tersedia dalam jenis algoritma kotak hitam seperti Neural Network. Waktu pelatihannya lebih cepat dibandingkan dengan algoritma jaringan saraf. Kompleksitas waktu pohon keputusan adalah fungsi dari jumlah catatan dan jumlah atribut dalam data yang diberikan. Pohon keputusan adalah metode distribusi-bebas atau non-parametrik, yang tidak bergantung pada asumsi distribusi probabilitas. Pohon keputusan dapat menangani data dimensi tinggi dengan akurasi yang baik.</p>
<h2 id="bagaimana-cara-kerja-algoritma-pohon-keputusan">Bagaimana cara kerja algoritma Pohon Keputusan?<a class="headerlink" href="#bagaimana-cara-kerja-algoritma-pohon-keputusan" title="Permanent link">&para;</a></h2>
<p>Gagasan dasar di balik algoritma pohon keputusan adalah sebagai berikut:</p>
<ol>
<li>Pilih atribut terbaik menggunakan Attribution Selection Measures (ASM) untuk membagi catatan.</li>
<li>Jadikan atribut itu sebagai simpul keputusan dan pisahkan dataset menjadi himpunan bagian yang lebih kecil.</li>
<li>Mulailah membangun pohon dengan mengulangi proses ini secara rekursif untuk setiap anak sampai salah satu dari kondisi tersebut akan cocok:</li>
<li>Semua tupel memiliki nilai atribut yang sama.</li>
<li>Tidak ada lagi atribut yang tersisa.</li>
<li>Tidak ada contoh lagi.</li>
</ol>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\2.JPG" /></p>
<h2 id="pengukuran-pilihan-atribut">Pengukuran Pilihan Atribut<a class="headerlink" href="#pengukuran-pilihan-atribut" title="Permanent link">&para;</a></h2>
<p>Ukuran pemilihan atribut adalah heuristik untuk memilih kriteria pemisahan yang membagi data menjadi cara terbaik. Ia juga dikenal sebagai aturan pemisahan karena membantu kita untuk menentukan breakpoints untuk tuple pada node yang diberikan. ASM memberikan peringkat untuk setiap fitur (atau atribut) dengan menjelaskan dataset yang diberikan. Atribut skor terbaik akan dipilih sebagai atribut pemisahan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=http://www.ijoart.org/docs/Construction-of-Decision-Tree--Attribute-Selection-Measures.pdf&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhhWuJ7IYdiEjfFxdu3QYzzGOOPWAw">Sumber</a> ). Dalam kasus atribut bernilai kontinu, titik perpecahan untuk cabang juga perlu ditentukan. Ukuran seleksi yang paling populer adalah Information Gain, Gain Ratio, dan Gini Index.</p>
<h2 id="keuntungan-informasi">Keuntungan Informasi<a class="headerlink" href="#keuntungan-informasi" title="Permanent link">&para;</a></h2>
<p>Shannon menemukan konsep entropi, yang mengukur ketidakmurnian set input.Dalam fisika dan matematika, entropi disebut sebagai keacakan atau ketidakmurnian dalam sistem. Dalam teori informasi, ini mengacu pada ketidakmurnian dalam sekelompok contoh. Keuntungan informasi adalah berkurangnya entropi. Informasi gain menghitung perbedaan antara entropi sebelum split dan rata-rata entropi setelah split dari dataset berdasarkan nilai atribut yang diberikan. Algoritma decision tree ID3 (Iterative Dichotomiser) menggunakan informasi gain.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\3.JPG" /></p>
<p>Di mana, Pi adalah probabilitas bahwa tuple arbitrer di D milik kelas Ci.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\4.JPG" /></p>
<p>Dimana,</p>
<ul>
<li>Info (D) adalah jumlah rata-rata informasi yang diperlukan untuk mengidentifikasi label kelas tuple di D.</li>
<li>| Dj | / | D | bertindak sebagai bobot partisi j.</li>
<li>InfoA (D) adalah informasi yang diharapkan diperlukan untuk mengklasifikasikan tuple dari D berdasarkan pada partisi oleh A.</li>
</ul>
<p>Atribut A dengan gain informasi tertinggi, Gain (A), dipilih sebagai atribut pemisahan pada simpul N ().</p>
<h2 id="rasio-keuntungan">Rasio Keuntungan<a class="headerlink" href="#rasio-keuntungan" title="Permanent link">&para;</a></h2>
<p>Keuntungan informasi bias untuk atribut dengan banyak hasil. Itu berarti lebih suka atribut dengan sejumlah besar nilai yang berbeda. Misalnya, pertimbangkan atribut dengan pengidentifikasi unik seperti customer_ID memiliki nol info (D) karena partisi murni. Ini memaksimalkan perolehan informasi dan menciptakan partisi yang tidak berguna.</p>
<p>C4.5, peningkatan ID3, menggunakan ekstensi untuk mendapatkan informasi yang dikenal sebagai rasio keuntungan. Rasio perolehan menangani masalah bias dengan menormalkan perolehan informasi menggunakan Split Info. Implementasi Java dari algoritma C4.5 dikenal sebagai J48, yang tersedia di alat penambangan data WEKA.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\5.JPG" /></p>
<p>Dimana,</p>
<ul>
<li>| Dj | / | D | bertindak sebagai bobot partisi j.</li>
<li>v adalah jumlah nilai diskrit dalam atribut A.</li>
</ul>
<p>Rasio gain dapat didefinisikan sebagai</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\6.JPG" /></p>
<p>Atribut dengan rasio gain tertinggi dipilih sebagai atribut pemisahan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=http://www.enggjournals.com/ijcse/doc/IJCSE10-02-09-092.pdf&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhheQH1gluzDJt_8XElHGXPzW_2NxQ">Sumber</a> ).</p>
<h2 id="indeks-gini">Indeks gini<a class="headerlink" href="#indeks-gini" title="Permanent link">&para;</a></h2>
<p>Algoritma decision tree lain, CART (Classification and Regression Tree) menggunakan metode Gini untuk membuat poin split.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\7.JPG" /></p>
<p>Di mana, pi adalah probabilitas bahwa sebuah tuple dalam D milik kelas Ci.</p>
<p>Indeks Gini mempertimbangkan pemisahan biner untuk setiap atribut. Anda dapat menghitung jumlah pembobolan dari setiap partisi. Jika pemisahan biner pada atribut A data partisi D menjadi D1 dan D2, indeks Gini D adalah:</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\8.JPG" /></p>
<p>Dalam hal atribut bernilai diskrit, subset yang memberikan indeks gini minimum untuk yang dipilih dipilih sebagai atribut pemisahan. Dalam kasus atribut bernilai kontinu, strateginya adalah memilih setiap pasangan nilai yang berdekatan sebagai titik perpecahan dan titik yang mungkin dengan indeks gini yang lebih kecil dipilih sebagai titik pemisahan.</p>
<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\9.JPG" /></p>
<p>Atribut dengan indeks minimum Gini dipilih sebagai atribut pemisahan.</p>
<h1 id="gedung-pengklasifikasian-pohon-keputusan-di-scikit-learn">Gedung Pengklasifikasian Pohon Keputusan di Scikit-learn<a class="headerlink" href="#gedung-pengklasifikasian-pohon-keputusan-di-scikit-learn" title="Permanent link">&para;</a></h1>
<h2 id="mengimpor-perpustakaan-yang-diperlukan">Mengimpor Perpustakaan yang Diperlukan<a class="headerlink" href="#mengimpor-perpustakaan-yang-diperlukan" title="Permanent link">&para;</a></h2>
<p>Pertama mari kita memuat perpustakaan yang diperlukan</p>
<pre class="codehilite"><code class="language-python"># Load libraries import pandas as pd from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier from sklearn.model_selection import train_test_split # Import train_test_split function from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation</code></pre>

<h2 id="memuat-data">Memuat data<a class="headerlink" href="#memuat-data" title="Permanent link">&para;</a></h2>
<p>Pertama mari kita memuat dataset Pima Indian Diabetes yang dibutuhkan menggunakan fungsi baca CSV panda. Anda dapat mengunduh data di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://www.kaggle.com/uciml/pima-indians-diabetes-database&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhigZ5cWoPmsoi5dcie4Pk1lEVCClA">sini</a> .</p>
<pre class="codehilite"><code class="language-python">col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label'] # load dataset pima = pd.read_csv("pima-indians-diabetes.csv", header=None, names=col_names) </code></pre>

<pre class="codehilite"><code class="language-python"> pima.head() </code></pre>

<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\10.JPG" /></p>
<h2 id="pemilihan-fitur">Pemilihan Fitur<a class="headerlink" href="#pemilihan-fitur" title="Permanent link">&para;</a></h2>
<p>Di sini, Anda perlu membagi kolom yang diberikan menjadi dua jenis variabel dependen (atau variabel target) dan variabel independen (atau variabel fitur).</p>
<pre class="codehilite"><code class="language-python"> #split dataset in features and target variable feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree'] X = pima[feature_cols] # Features y = pima.label # Target variable </code></pre>

<h2 id="memisahkan-data">Memisahkan Data<a class="headerlink" href="#memisahkan-data" title="Permanent link">&para;</a></h2>
<p>Untuk memahami kinerja model, membagi dataset ke dalam set pelatihan dan set tes adalah strategi yang baik.</p>
<p>Mari kita pisahkan dataset dengan menggunakan function train_test_split (). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set.</p>
<pre class="codehilite"><code class="language-python"># Split dataset into training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test </code></pre>

<h2 id="membangun-model-pohon-keputusan">Membangun Model Pohon Keputusan<a class="headerlink" href="#membangun-model-pohon-keputusan" title="Permanent link">&para;</a></h2>
<p>Mari kita membuat Model Pohon Keputusan menggunakan Scikit-belajar.</p>
<pre class="codehilite"><code class="language-python"> # Create Decision Tree classifer object clf = DecisionTreeClassifier() # Train Decision Tree Classifer clf = clf.fit(X_train,y_train) #Predict the response for test dataset y_pred = clf.predict(X_test) </code></pre>

<h2 id="mengevaluasi-model">Mengevaluasi Model<a class="headerlink" href="#mengevaluasi-model" title="Permanent link">&para;</a></h2>
<p>Mari kita perkirakan, seberapa akurat classifier atau model dapat memprediksi jenis kultivar.</p>
<p>Akurasi dapat dihitung dengan membandingkan nilai set tes aktual dan nilai prediksi.</p>
<pre class="codehilite"><code class="language-python"> # Model Accuracy, how often is the classifier correct? print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) </code></pre>

<pre class="codehilite"><code class="language-python">Accuracy: 0.6753246753246753 </code></pre>

<p>Nah, Anda mendapat tingkat klasifikasi 67,53%, dianggap sebagai akurasi yang baik. Anda dapat meningkatkan akurasi ini dengan menyetel parameter dalam Decision Tree Algorithm.</p>
<h2 id="memvisualisasikan-pohon-keputusan">Memvisualisasikan Pohon Keputusan<a class="headerlink" href="#memvisualisasikan-pohon-keputusan" title="Permanent link">&para;</a></h2>
<p>Anda dapat menggunakan fungsi <em>export_graphviz</em> dari <em>Scikit</em> - <em>learn</em> untuk menampilkan pohon dalam notebook Jupyter. Untuk merencanakan pohon, Anda juga perlu menginstal graphviz dan pydotplus.</p>
<pre class="codehilite"><code class="language-python">pip install graphviz</code></pre>

<pre class="codehilite"><code>pip install pydotplus</code></pre>

<p>Fungsi <em>export_graphviz</em> mengubah classifier pohon keputusan menjadi file dot dan pydotplus mengubah file dot ini menjadi png atau bentuk yang dapat ditampilkan di Jupyter.</p>
<pre class="codehilite"><code class="language-python">from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image import pydotplus dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['0','1']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('diabetes.png') Image(graph.create_png()) </code></pre>

<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\11.JPG" /></p>
<p>Dalam bagan pohon keputusan, setiap simpul internal memiliki aturan keputusan yang membagi data. Gini disebut sebagai rasio Gini, yang mengukur ketidakmurnian simpul. Anda dapat mengatakan bahwa sebuah simpul adalah murni ketika semua catatan milik kelas yang sama, simpul tersebut dikenal sebagai simpul daun.</p>
<p>Di sini, pohon yang dihasilkan tidak dilepas. Pohon tanpa ikatan ini tidak bisa dijelaskan dan tidak mudah dipahami. Di bagian selanjutnya, mari optimalkan dengan pemangkasan.</p>
<h2 id="mengoptimalkan-kinerja-pohon-keputusan">Mengoptimalkan Kinerja Pohon Keputusan<a class="headerlink" href="#mengoptimalkan-kinerja-pohon-keputusan" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>kriteria: opsional (default = ”gini”) atau Pilih ukuran pemilihan atribut</strong> : Parameter ini memungkinkan kita untuk menggunakan ukuran pemilihan atribut yang berbeda-beda. Kriteria yang didukung adalah "gini" untuk indeks Gini dan "entropi" untuk perolehan informasi.</li>
<li><strong>splitter: string, opsional (default = ”best”) atau Split Strategy</strong> : Parameter ini memungkinkan kita untuk memilih strategi split. Strategi yang didukung adalah "terbaik" untuk memilih split terbaik dan "acak" untuk memilih split acak terbaik.</li>
<li><strong>max_depth: int atau None, opsional (default = None) atau Maximum Depth of a Tree</strong> : Kedalaman maksimum pohon. Jika tidak ada, maka node diperluas hingga semua daun mengandung kurang dari sampel min_samples_split. Nilai kedalaman maksimum yang lebih tinggi menyebabkan overfitting, dan nilai yang lebih rendah menyebabkan underfitting ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhg6r5XzeS1EQ-N155gx7MZPhJmdmw">Sumber</a> ).</li>
</ul>
<p>Dalam Scikit-belajar, optimalisasi classifier pohon keputusan dilakukan hanya dengan pra-pemangkasan. Kedalaman maksimum pohon dapat digunakan sebagai variabel kontrol untuk pra-pemangkasan. Dalam contoh berikut ini, Anda bisa memplot pohon keputusan pada data yang sama dengan max_depth = 3. Selain parameter pra-pemangkasan, Anda juga dapat mencoba ukuran pemilihan atribut lainnya seperti entropi.</p>
<pre class="codehilite"><code class="language-python"># Create Decision Tree classifer object clf = DecisionTreeClassifier(criterion="entropy", max_depth=3) # Train Decision Tree Classifer clf = clf.fit(X_train,y_train) #Predict the response for test dataset y_pred = clf.predict(X_test) # Model Accuracy, how often is the classifier correct? print("Accuracy:",metrics.accuracy_score(y_test, y_pred)) </code></pre>

<pre class="codehilite"><code class="language-python"> Accuracy: 0.7705627705627706 </code></pre>

<p>Nah, tingkat klasifikasi meningkat menjadi 77,05%, yang merupakan akurasi yang lebih baik daripada model sebelumnya.</p>
<h2 id="memvisualisasikan-pohon-keputusan_1">Memvisualisasikan Pohon Keputusan<a class="headerlink" href="#memvisualisasikan-pohon-keputusan_1" title="Permanent link">&para;</a></h2>
<pre class="codehilite"><code class="language-python">from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True, feature_names = feature_cols,class_names=['0','1']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('diabetes.png') Image(graph.create_png()) </code></pre>

<p><img alt="" src="D:\KULIAH\Data Mining\mkdocs-material-master\docs\assets\images\12.JPG" /></p>
<p>Model pemangkasan ini tidak terlalu rumit, dapat dijelaskan, dan mudah dipahami daripada plot model pohon keputusan sebelumnya.</p>
<h2 id="pro">Pro<a class="headerlink" href="#pro" title="Permanent link">&para;</a></h2>
<ul>
<li>Pohon keputusan mudah diinterpretasikan dan divisualisasikan.</li>
<li>Ini dapat dengan mudah menangkap pola Non-linear.</li>
<li>Ini membutuhkan lebih sedikit pemrosesan data dari pengguna, misalnya, tidak perlu menormalkan kolom.</li>
<li>Ini dapat digunakan untuk rekayasa fitur seperti memprediksi nilai yang hilang, cocok untuk pemilihan variabel.</li>
<li>Pohon keputusan tidak memiliki asumsi tentang distribusi karena sifat non-parametrik dari algoritma. ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://scikit-learn.org/stable/modules/tree.html&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhgSTriHyPAxa5abHrVqCtaxC53kOw">Sumber</a> )</li>
</ul>
<h2 id="cons">Cons<a class="headerlink" href="#cons" title="Permanent link">&para;</a></h2>
<ul>
<li>Peka terhadap data berisik. Itu bisa menyesuaikan data yang bising.</li>
<li>Variasi kecil (atau varians) dalam data dapat menghasilkan pohon keputusan yang berbeda. Ini dapat dikurangi dengan mengantongi dan meningkatkan algoritma.</li>
<li>Pohon keputusan bias dengan dataset ketidakseimbangan, jadi disarankan untuk menyeimbangkan dataset sebelum membuat pohon keputusan.</li>
</ul>
<h2 id="kesimpulan">Kesimpulan<a class="headerlink" href="#kesimpulan" title="Permanent link">&para;</a></h2>
<p>Selamat, Anda telah berhasil sampai akhir tutorial ini!</p>
<p>Dalam tutorial ini, Anda membahas banyak detail tentang Decision Tree; Ini bekerja, langkah-langkah pemilihan atribut seperti Information Gain, Gain Ratio, dan Gini Index, pembuatan model pohon keputusan, visualisasi dan evaluasi pada dataset diabetes menggunakan paket Python Scikit-learning. Juga membahas pro, kontra, dan mengoptimalkan kinerja Decision Tree menggunakan penyetelan parameter.</p>
<p>Mudah-mudahan, sekarang Anda dapat menggunakan algoritma pohon Keputusan untuk menganalisis kumpulan data Anda sendiri.</p>
<p>Jika Anda ingin mempelajari lebih lanjut tentang Pembelajaran Mesin dengan Python, ambil <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhiralOoM8nWJjvL96pGmf0KVafpWQ">Pembelajaran Mesin</a> DataCamp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhiralOoM8nWJjvL96pGmf0KVafpWQ">dengan Model Berbasis Pohon dalam</a> kursus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;hl=en&amp;rurl=translate.google.com&amp;sl=en&amp;sp=nmt4&amp;tl=id&amp;u=https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python&amp;xid=17259,15700022,15700186,15700190,15700256,15700259&amp;usg=ALkJrhiralOoM8nWJjvL96pGmf0KVafpWQ">Python</a> .</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="K-Nearest Neighbors" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                K-Nearest Neighbors
              </span>
            </div>
          </a>
        
        
          <a href="../penyusun/" title="Penyusun" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Penyusun
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 Ainun Nawawi
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/ainunnawawi" class="md-footer-social__link fa fa-github-alt"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>